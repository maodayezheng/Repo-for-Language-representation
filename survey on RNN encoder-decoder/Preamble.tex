\maketitle
\makedeclaration

\begin{abstract} % 300 word limit
Learning the vector representations for sentences is a tough problem, although there are some models can learn such representations, it is still not clear about which architectures are preferred and what objectives can result in most useful representations. 
This thesis presents a comparison between several recurrent neural network based encoders have exactly same architecture but are trained with different objectives.\\\\
We first describe how we pretrain recent proposed sequence to sequence model on different objectives to obtain sentence encoders. 
Next we introduce metrics that are used to evaluate these encoders.
Finally, we discuss our evaluation results and figure out a few points that can affect the performance of these encoders.
\end{abstract}

\begin{acknowledgements}
Firstly, I would like to thank my supervisor, Dr David Barber,
for his patience and continuous support during this year. 
He makes me realise the basis of doing research is to be precise. 
Furthermore, I would like to thank all members of my study group, without your help it would be really difficult for me to develop knowledge about machine learning from knowing nothing.
And I also would like to thank UCL Media Future group to share their computational resource with me.
Finally, thank for those encouragements from my parents and Miss Kejing when I feel struggling this year.
\end{acknowledgements}

\setcounter{tocdepth}{2} 
% Setting this higher means you get contents entries for
%  more minor section headers.

\tableofcontents
\listoffigures
\listoftables

